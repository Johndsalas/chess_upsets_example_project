{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "passive-collect",
   "metadata": {},
   "source": [
    "# Goal: \n",
    "* Discover drivers of upsets in chess games played on Lichess.org\n",
    "* Use those drivers to develop a machine learning model to predict whether a game will end in upset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-insertion",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "following-steam",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy import stats\n",
    "import re\n",
    "\n",
    "import wrangle as w\n",
    "import explore as e\n",
    "import modeling as m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-swimming",
   "metadata": {},
   "source": [
    "# Acquire\n",
    "\n",
    "* Data acquired from [Kaggle](https://www.kaggle.com/datasnaek/chess)\n",
    "* It contained 20,058 rows and 9 columns before cleaning\n",
    "* Each row represents a chess game played on Lichess.org\n",
    "* Each column represents a feature of those games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-steering",
   "metadata": {},
   "source": [
    "# Prepare\n",
    "\n",
    "**Prepare Steps:**\n",
    "* Removed columns that did not contain useful information\n",
    "* Renamed columns to premote readability\n",
    "* Checked for nulls in the data (there were none)\n",
    "* Checked that column data types were apropriate\n",
    "* Removed white space from values in object columns\n",
    "* Added Target column 'upset' indicating weather the lower rated player won the game\n",
    "* Added additional features to investigate:\n",
    "    * rating_difference\n",
    "    * game_rating\n",
    "    * lower_rated_white\n",
    "    * time_control_group\n",
    "* Encoded 'Time Control Group' and 'Opening Name' as binary features for each column\n",
    "* Split data into train, validate and test (approx. 60/25/15), stratifying on 'upset'\n",
    "* Outliers have not been removed for this itteration of the project\n",
    "\n",
    "\n",
    "* Split train, validate, and test into X and y dataframes\n",
    "* Scaled continuous variable\n",
    "* Encoded  variables\n",
    "* converted 'Upset' to catagorical variable with values upset or non-upset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-plymouth",
   "metadata": {},
   "source": [
    "# Data Dictionary\n",
    "\n",
    "| Feature | Definition |\n",
    "|:--------|:-----------|\n",
    "|Rated| True or False, The game's result is reflected in each player's rating|\n",
    "|Winning Pieces| The color of pieces the winning player was moving|\n",
    "|White Rating| Rating of the player moving the white pieces using the Glicko-2 rating method for games played on Lichess|\n",
    "|Black Rating| Rating of the player moving the white pieces using the Glicko-2 rating method for games played on Lichess|\n",
    "|Rating Difference| The difference in rating between the players in the game|\n",
    "|Game Rating| The average rating of the two players in the game|\n",
    "|Lower Rated White| True or False, The lower rated player is moving the white pieces|\n",
    "|Opening Name| The name of the opening played in the game|\n",
    "|Time Control Group| The amount of time aloted to each player to make their moves, **Standard** (60 min or more), **Rapid** (30 - 15 min), **Blitz** (5 - 3 min), or **Bullet** (2 or less), **Other** (any other time limit)|\n",
    "|Upset (Target)| True or False, The lower rated player won the game|\n",
    "|Additional Features|Encoded values for 'Time Control Group' and 'Opening Name'|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pressing-legislation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000000    55\n",
       "0.010724    50\n",
       "0.006032    48\n",
       "0.014745    45\n",
       "0.002681    44\n",
       "            ..\n",
       "0.469839     1\n",
       "0.426944     1\n",
       "0.331769     1\n",
       "0.290885     1\n",
       "0.297587     1\n",
       "Name: rating_difference_scaled, Length: 721, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acquiring, cleaning, and adding features to data\n",
    "df = w.wrangle_chess_data(reprep = True)\n",
    "\n",
    "# splitting data into train, validate, and test\n",
    "train, validate, test = w.split_my_data(df)\n",
    "\n",
    "# adding scaled columns of continuous features\n",
    "train, validate, test = w.scale_data(train, validate, test)\n",
    "\n",
    "train.rating_difference_scaled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "municipal-donna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time_control_group', 'rated', 'winning_pieces', 'increment_code',\n",
       "       'white_rating', 'black_rating', 'opening_name', 'upset',\n",
       "       'rating_difference', 'game_rating', 'lower_rated_white',\n",
       "       'time_control_group_Blitz', 'time_control_group_Bullet',\n",
       "       'time_control_group_Other', 'time_control_group_Rapid',\n",
       "       'time_control_group_Standard', 'rating_difference_scaled',\n",
       "       'game_rating_scaled'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "blind-crazy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating_difference</th>\n",
       "      <th>rating_difference_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>11</td>\n",
       "      <td>0.098525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5159</th>\n",
       "      <td>238</td>\n",
       "      <td>0.047587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14725</th>\n",
       "      <td>371</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13305</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14307</th>\n",
       "      <td>345</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5771</th>\n",
       "      <td>213</td>\n",
       "      <td>0.166890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7838</th>\n",
       "      <td>136</td>\n",
       "      <td>0.274799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8204</th>\n",
       "      <td>435</td>\n",
       "      <td>0.030161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14144</th>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19724</th>\n",
       "      <td>79</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11232 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rating_difference  rating_difference_scaled\n",
       "3596                  11                  0.098525\n",
       "5159                 238                  0.047587\n",
       "14725                371                       NaN\n",
       "13305                  4                       NaN\n",
       "14307                345                       NaN\n",
       "...                  ...                       ...\n",
       "5771                 213                  0.166890\n",
       "7838                 136                  0.274799\n",
       "8204                 435                  0.030161\n",
       "14144                 44                       NaN\n",
       "19724                 79                       NaN\n",
       "\n",
       "[11232 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['rating_difference','rating_difference_scaled']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-paper",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-attendance",
   "metadata": {},
   "source": [
    "## How often do upsets occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pie chart upsets\n",
    "e.get_pie_upsets(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-committee",
   "metadata": {},
   "source": [
    "* About 1/3 of the games in the training data will end in upset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-gothic",
   "metadata": {},
   "source": [
    "## Dose first move advantage effect upsets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-environment",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get pie chart lower rated white\n",
    "e.get_pies_white(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-nursing",
   "metadata": {},
   "source": [
    "* Upset percentage is 4% higher in games where the lower rated player makes the first move."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-wilderness",
   "metadata": {},
   "source": [
    "**I will now use a chi-square test to investigate whether 'Upset' and \"Lower Rated White\" are related** \n",
    "* Traditionally the player moving the white pieces moves first\n",
    "* I will use a confidance interval of 95% \n",
    "* The resulting alpha is .05<br>\n",
    "\n",
    "**Ho: \"Upset\" and \"Lower Rated White\" are independant of one another.**<br>\n",
    "**Ha: \"Upset\" and \"Lower Rated White\" are related.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get chi-square test\n",
    "e.get_chi_white(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-brook",
   "metadata": {},
   "source": [
    "**The p-value is grater than the alpha. Therefore, we have evidence to support that \"Upset\" and \"Lower Rated White\" are related. Based on this, and the 4% difference in upsets, observed in the train data, I believe that using the \"Lower Rated White\" feature in modeling will likely have a small positive impact on the model's accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-longitude",
   "metadata": {},
   "source": [
    "## Does a game being rated effect upsets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-encyclopedia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get pie charts\n",
    "e.get_pie_rated(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-bulgaria",
   "metadata": {},
   "source": [
    "* Upset percentage is 3% higher in games that are rated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-presence",
   "metadata": {},
   "source": [
    "**I will now use a chi-square test to investigate weather \"Upset\" and \"Rated\" are related.**\n",
    "* I will use a confidance interval of 95% \n",
    "* The resulting alpha is .05<br>\n",
    "\n",
    "**Ho: \"Rated\" and \"Upset\" are independant of one another.** <br>\n",
    "**Ha: \"Rated\" and \"Upset\" are related.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get chi-square results\n",
    "e.get_chi_rated(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-tsunami",
   "metadata": {},
   "source": [
    "**The p-value is less than the alpha. Therefore, we have evidence to support that \"Upset\" and \"Rated\" are related. Based on this and the 3% difference in upsets, observed in the train data, I believe that using the \"Rated\" feature in modeling will likely have a small positive impact on the model's accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-finance",
   "metadata": {},
   "source": [
    "## Does player rating have an effect on upsets?\n",
    "I will examine two subquestions to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-preference",
   "metadata": {},
   "source": [
    "### 1) Does game rating (The average rating of both players in a game) have an effect on upsets? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bar chart\n",
    "e.get_game_rating(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-publicity",
   "metadata": {},
   "source": [
    "* The average game rating for upsets is very similer to the average game rating for non-upsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-blues",
   "metadata": {},
   "source": [
    "**Because the average game rating for games that end in upsets is very similer to the average game rating of games that do not end in upsets, it is not likely that \"Game Rating\" will be a useful feature to model on.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-noise",
   "metadata": {},
   "source": [
    "### 2) Does difference in player rating have an effect on upsets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bar chart\n",
    "e.ave_diff_rating(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-oasis",
   "metadata": {},
   "source": [
    "* The average difference in player rating is 82 points lower in games ending in upset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-sentence",
   "metadata": {},
   "source": [
    "**I will now do a T-test to test for a significant difference between the mean difference in player rating of games ending in upset and the mean difference in player rating of games ending in non-upset.**\n",
    "\n",
    "* I will use a confidence interval of 95%\n",
    "* The resulting alpha is .05\n",
    "\n",
    "**HO: The mean difference in player rating of games ending in upset is not significantly differint from the mean difference in player rating of games not ending in an upset.** <br>\n",
    "**HA: The mean difference in player rating of games ending in upset is significantly differint from the mean difference in player rating of games not ending in an upset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get T-test resul\n",
    "e.get_t_rating_diff(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-hammer",
   "metadata": {},
   "source": [
    "**The p-value is less than the alpha. Therefore, we have evidence to support that the mean rating difference of players in games ending in upset is significantly differint from the mean rating difference of players in games that end in non-upsets. Based on this, and the 82 point difference in means, observed in the train data, I believe that using \"Rating Difference\" during modeling will provide a moderate improvement in the model's accuracy.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-catch",
   "metadata": {},
   "source": [
    "## Does time block effect upsets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-attraction",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get pie charts\n",
    "e.get_pie_time(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-arabic",
   "metadata": {},
   "source": [
    "* In time control groups where time is very limited, such as Bullet, Blitz, and Rapid games upset percentage ranges from 30 to 34%.\n",
    "* In Standard, where time is more plentaful, upsets drop to 22%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-playback",
   "metadata": {},
   "source": [
    "**I will now perform a chi-square test to determin if \"Upset\" and \"Time control Group\" are independant.**\n",
    "* I will use a confidence interval of 95%\n",
    "* The resulting alpha is .05\n",
    "\n",
    "**Ho: \"Upset\" and \"Time Control Group\" are independant of one another.** <br>\n",
    "**Ha: \"Upset\" and \"Time Control Group\" are related.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get chi-square test\n",
    "e.get_chi_time(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-being",
   "metadata": {},
   "source": [
    "**The p-value is less than the alpha. Therefore, we have evidence to support that \"Time Control Group\" and \"Upset\" are related. Based on this, and the differences in upset percentages amoung the differint time groups, I belive that a having a standard time control is a driver of upsets. Adding an encoded version of this feature to the model will likely have a moderate positive effect on the model's accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-cinema",
   "metadata": {},
   "source": [
    "# Does Opening effect upsets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-bahrain",
   "metadata": {},
   "source": [
    "* There are 1236 unique openings identified in the training data\n",
    "* This is too many for a thorough examination of each\n",
    "* I will examin the top ten, by popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-reducing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get upset distributions of top ten most populer openings\n",
    "e.get_pie_open(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-competition",
   "metadata": {},
   "source": [
    "* Percentage of upsets range from 20 - 37 amoung top openings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-tradition",
   "metadata": {},
   "source": [
    "**I will now run a chi-square test to see if \"Opening Name\" and \"Upset\" are dependant on one another.**\n",
    "* I will use a confidence interval of 95%\n",
    "* The resulting alpha is .05\n",
    "\n",
    "**Ho: \"Opening Name\" and \"Upset\" are independant of one another.** <br>\n",
    "**Ha: \"Opening Name\" and \"Upset\" are dependant on one another.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get chi-square results\n",
    "e.get_chi_open(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-edinburgh",
   "metadata": {},
   "source": [
    "**The p-value is less than the Alpha. Therfore, we have evidence to support that \"Opening Name\" and \"Upset\" are related. However, their are over 1236 unique openings in opening_names. Adding that number of endoded columns to the model would likely do more harm than good.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-industry",
   "metadata": {},
   "source": [
    "# Exploration Summery\n",
    "\n",
    "* \"Lower Rated White\" and \"Rated\" were each found to be drivers of \"Upset\"\n",
    "    * Though the amount of influance each has is likely to be weak\n",
    "* \"Rating Difference\" was found to be a driver of \"Upset\"\n",
    "* \"Time Control Group\" was found to be a driver of \"Upsets\" \n",
    "    * Being in the standard time control or not seemed to have a particularly strong influance.\n",
    "* \"Opening Name\" was found to be a driver of \"Upsets\" \n",
    "    * Upset percentage ranged from 20-37%\n",
    "    * encoding all of these features would result in more noise than signal to the model\n",
    "    * It may be possible to create groups of simmiler openings in order to make a more resonable number features\n",
    "\n",
    "</br>\n",
    "* \"Game Rating\" was not found to be a driver of upsets\n",
    "<br>\n",
    "\n",
    "# Features I am moving to modeling With\n",
    "* \"Lower Rated White\" (small difference in upset percentage, but relationship to upsets is statistically significant)\n",
    "* \"Rated\" (small difference in upset percentage, but relationship to upsets is statistically significant)\n",
    "* \"Time Control standard\" (moderate difference in upset percentage, and dependance is statistically significant)\n",
    "* \"Rating Difference\" (Large difference in rating observed, and difference is significant)\n",
    "\n",
    "# Features I'm not moving to modeling with\n",
    "* \"Opening\" (Although found to be a driver of \"upset\" the encoding process would result in more noise than signal at this time)\n",
    "* \"Game Rating\" (There is no evidence that \"Game Rating\" is a driver of upsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-halifax",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "* I will use accuracy as my evaluation metric  \n",
    "* non-upsets makeup 67% of the data \n",
    " <br>\n",
    "* by guessing non-upset for every game one could achieve an accuracy of 67%\n",
    "* 67% will be the baseline accuracy I use for this project\n",
    " <br>\n",
    "* I will be evaluating models developed using four differint model types and various hyperperamiter configurations \n",
    "* Models will be evaluated on train and validate data\n",
    "* The model that performs the best will then be evaluated on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep data for modeling\n",
    "train_X, validate_X, test_X, train_y, validate_y, test_y = m.model_prep(train,validate,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-ceramic",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get decision tree results\n",
    "m.get_tree(train_X, validate_X, train_y, validate_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-pottery",
   "metadata": {},
   "source": [
    "* Deceision Tree accuracy is about equal to the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-strength",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random forest results\n",
    "m.get_forest(train_X, validate_X, train_y, validate_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-oxygen",
   "metadata": {},
   "source": [
    "* Random Forest accuracy is about equal to the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-algorithm",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get logistic regression results\n",
    "m.get_reg(train_X, validate_X, train_y, validate_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-station",
   "metadata": {},
   "source": [
    "* Logistic regression accuracy is better than baseline on train, and worse thatn baseline on validate\n",
    "* It is likely over-fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-india",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get knn results\n",
    "m.get_knn(train_X, validate_X, train_y, validate_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-jewel",
   "metadata": {},
   "source": [
    "* Logistic regression accuracy is better than baseline on train, and worse than baseline on validate\n",
    "* It is likely over-fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-denmark",
   "metadata": {},
   "source": [
    "# Compairing Models\n",
    "\n",
    "* All models perform at or below baseline\n",
    "* The two models that perform at baseline are Decision Tree and Random Forest\n",
    "* Because both are within rounding error of one another in terms of accuracy\n",
    "* I will proceed with the model that requiers the least amount of processing to run\n",
    "* I will proceed to test with a Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-screen",
   "metadata": {},
   "source": [
    "# Decision Tree on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test results for final model\n",
    "m.get_tree_test(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-wallace",
   "metadata": {},
   "source": [
    "### Modeling Summery\n",
    "\n",
    "* Decision Tree and Random Rorest models had an accuracy of about the same as the baseline\n",
    "* Logistic Regression and KNN models out performed the baseline accuracy on train data, but the data was overfit leading the models having a worse than baseline accuracy on validate data\n",
    "* A Decision Tree was selected as the final model and had an accuracy of 67% which is about equal to the baseline accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-initial",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "### Exploration\n",
    "\n",
    "* Upsets in chess occur in about 1/3 of games\n",
    "* Games in which the lower rated player makes the first move, and games that are rated have a slightly higher chance of ending in an upset \n",
    "* Games ending in upset have a much lower mean difference in player rating than games not ending in upset\n",
    "* Games using shorter time control, such as Bullet, Blitz, and Rapid games, have an upset percentage that closely mirrors the overall upset percentage ranging from 30-34% while standard games have a much lower upset percentage at 22%\n",
    "* Looking at the top 10 openings in terms of frequency in the data set we can conclude that a given opening does affect the likelihood of a game ending in an upset. Upset percentages very by opening from 20-39%\n",
    "* The average rating of players in a game has no provable effect on the chance of that game ending in upset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-oriental",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "**All of the models failed to outperform the baseline. Possible reasons and solutions include:**\n",
    "\n",
    "* “Rated” and “lower rated white” each only accounted for a small difference in the percentage of upsets\n",
    "\n",
    "* Time Control group had one outlier, in terms of percentage of upsets, which was standard. The values of each of the other time control groups were much closer together. Perhaps modeling only on this time controle would remove noise from the model.\n",
    "\n",
    "* While “Opening Name” seemed to be a significant driver of upsets. It contains 1200+ values, that once encoded would adds an overwhilming number of features to the model. The additional noise this would add may have done more harm than good to the model. \n",
    "\n",
    "* Finally the values in “Opening Name” seem to include a major opening name along with variations of those openings. Creating clusters of all the variations of each opening may result in a more manageable number of features.   \n",
    "\n",
    "**Should I have ocation to revisit this project I would like to try the following:**\n",
    "\n",
    "* Cluster together opening variants in \"Opening Name\" to reduce the number of features input into the model\n",
    "* Run the models without \"Opening Name\" to see there is any improvement made by just removing the additional noise.\n",
    "* Look for other ways to describe \"Opening Name\" Such as by popularity of the opening or average rating of players playing that opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-tiger",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
